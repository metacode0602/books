# 使用LlamaIndex实现图片搜索

通常情况下，理解一张图片需要从知识库中查找信息。这里有一个流程是检索增强型图像描述生成——首先使用多模态模型对图片进行描述性标注，然后通过从一个文本语料库中检索来精炼这个描述。

具体来说，这个过程包括以下几个步骤：

1. **多模态模型标注**：使用一个能够处理图像和文本的多模态模型来生成图片的初步描述。

2. **文本语料库检索**：将生成的初步描述与一个大型文本语料库进行匹配，以检索出更加准确或信息量更大的描述。

3. **描述精炼**：根据检索到的信息，对初步描述进行修改和补充，以生成最终的、更加精炼的图像描述。

这个过程可以提高图像描述的准确性和信息量，尤其是在处理包含特定对象、场景或事件的复杂图像时。通过结合多模态学习和文本检索技术，检索增强型图像描述生成能够提供更丰富、更准确的图像内容解释。

在这个笔记本指南中，我们将演示如何评估一个多模态RAG系统。就像仅处理文本的情况一样，我们将分别考虑检索器和生成器的评估。正如我们在关于评估多模态RAGs的博客中所暗示的，我们这里的方法包括应用适应性的常用技术版本，这些技术通常用于评估检索器和生成器（用于仅文本的情况）。这些适应性的版本是llama-index库（即评估模块）的一部分，这个笔记本将引导你如何将它们应用到你的评估用例中。

### 评估多模态RAG系统的步骤概述：

1. **理解多模态RAG系统**：多模态RAG系统能够处理和生成图像、视频、音频以及文本等多种类型的数据。

2. **分别评估检索器和生成器**：评估过程将分别针对系统的两个主要组成部分——检索相关信息的检索器和基于检索到的信息生成输出的生成器。

3. **使用llama-index库**：利用llama-index库中的评估模块来适应性地修改和应用评估技术。

4. **评估技术的应用**：展示如何将这些技术应用到具体的评估场景中，以量化系统的性能。

5. **笔记本指南**：通过这个笔记本，用户可以学习如何将评估方法集成到他们自己的项目中，并对多模态RAG系统进行有效评估。

通过遵循这个指南，研究人员和开发者可以更好地理解和评估他们的多模态RAG系统的性能，并据此进行优化和改进。

我们将展示如何使用 LlamaIndex 构建一个图像到图像的检索系统，该系统结合了 GPT4-V 和 CLIP。

### LlamaIndex 图像到图像检索

- **图像嵌入索引**：使用 OpenAI 提供的 CLIP 嵌入技术来处理图像。
- **框架**：LlamaIndex。

### 步骤：

1. **下载文本、图像、PDF原始文件**：从维基百科页面下载这些资源。

2. **构建多模态索引和向量存储**：为文本和图像建立多模态索引和向量存储。

3. **使用多模态检索器检索相关图像**：给定一个图像查询，使用多模态检索器检索相关的图像。

4. **使用 GPT4V 推理输入图像与检索图像之间的相关性**：利用 GPT4V 对输入图像和检索到的图像之间的相关性进行推理。

CLIP（Contrastive Language-Image Pre-training）是一个由 OpenAI 开发的多模态模型，能够将图像和文本映射到同一个向量空间中，从而使得可以通过文本描述来检索图像，或者反过来，通过图像来检索相关的文本。

LlamaIndex 是一个用于构建索引和检索系统的框架，它支持多模态数据，可以用于提高检索任务的效率和准确性。

GPT4V（可能是指某个版本的 GPT 模型）在这里被用于推理两个图像之间的相关性，这可能是通过分析它们在 CLIP 嵌入空间中的向量关系来实现的。

这个笔记本向读者展示了如何结合这些先进的技术来实现图像检索任务，这在多媒体内容分析、图像推荐系统等领域有着广泛的应用。

### Ollama检索增强型图像描述  -- 多模态 RAG

#### 结构化数据提取自图像

这一部分可能介绍了如何从图像中提取结构化数据，例如识别图像中的特定对象、地标或文本，并将其转换成可操作的数据格式。

#### 检索增强型图像描述

这一部分可能讲解了如何利用检索增强型技术来改进图像描述生成。这通常涉及使用检索系统来找到与输入图像相关的信息，并利用这些信息来生成更准确或更丰富的描述。

#### 多模态 RAG

这部分可能探讨了多模态 RAG 的概念，即结合来自不同模态（如文本、图像、音频等）的信息来增强生成任务，例如在图像描述、问答或内容创作中的应用。

#### 安装依赖库

```shell
pip install llama-index-multi-modal-llms-ollama
pip install llama-index-readers-file
pip install unstructured
pip install llama-index-embeddings-huggingface
pip install llama-index-vector-stores-qdrant
pip install llama-index-embeddings-clip
```

**从图像中提取结构化数据** 在这里，我们将展示如何使用 LLaVa 从图像中提取信息到一个结构化的 Pydantic 对象中。

我们可以通过我们的 `MultiModalLLMCompletionProgram ` 来实现这一点。它通过一个提示模板、一组你想要提问的图像，以及期望的输出 Pydantic 对象来实例化。

```python
from llama_index.multi_modal_llms.ollama import OllamaMultiModal
mm_model = OllamaMultiModal(model="llava:13b")


from pathlib import Path
from llama_index.core import SimpleDirectoryReader
from PIL import Image
import matplotlib.pyplot as plt

input_image_path = Path("restaurant_images")
if not input_image_path.exists():
    Path.mkdir(input_image_path)

!wget "https://docs.google.com/uc?export=download&id=1GlqcNJhGGbwLKjJK1QJ_nyswCTQ2K2Fq" -O ./restaurant_images/fried_chicken.png

# load as image documents
image_documents = SimpleDirectoryReader("./restaurant_images").load_data()
# display image
imageUrl = "./restaurant_images/fried_chicken.png"
image = Image.open(imageUrl).convert("RGB")
plt.figure(figsize=(16, 5))
plt.imshow(image)
```

```python
from pydantic import BaseModel


class Restaurant(BaseModel):
    """Data model for an restaurant."""

    restaurant: str
    food: str
    discount: str
    price: str
    rating: str
    review: str



from llama_index.core.program import MultiModalLLMCompletionProgram
from llama_index.core.output_parsers import PydanticOutputParser

prompt_template_str = """\
{query_str}

Return the answer as a Pydantic object. The Pydantic schema is given below:

"""
mm_program = MultiModalLLMCompletionProgram.from_defaults(
    output_parser=PydanticOutputParser(Restaurant),
    image_documents=image_documents,
    prompt_template_str=prompt_template_str,
    multi_modal_llm=mm_model,
    verbose=True,
)


response = mm_program(query_str="Can you summarize what is in the image?")
for res in response:
    print(res)
```

在这里，我们展示了一个简单的检索增强型图像描述流水线示例，通过我们的查询流水线语法来表达。

```python
!wget "https://www.dropbox.com/scl/fi/mlaymdy1ni1ovyeykhhuk/tesla_2021_10k.htm?rlkey=qf9k4zn0ejrbm716j0gg7r802&dl=1" -O tesla_2021_10k.htm
!wget "https://docs.google.com/uc?export=download&id=1THe1qqM61lretr9N3BmINc_NWDvuthYf" -O shanghai.jpg

# from llama_index import SimpleDirectoryReader
from pathlib import Path
from llama_index.readers.file import UnstructuredReader
from llama_index.core.schema import ImageDocument


loader = UnstructuredReader()
documents = loader.load_data(file=Path("tesla_2021_10k.htm"))

image_doc = ImageDocument(image_path="./shanghai.jpg")
```

```python
from llama_index.core import VectorStoreIndex
from llama_index.core.embeddings import resolve_embed_model

embed_model = resolve_embed_model("local:BAAI/bge-m3")
vector_index = VectorStoreIndex.from_documents(
    documents, embed_model=embed_model
)
query_engine = vector_index.as_query_engine()
```

```python
from llama_index.core.prompts import PromptTemplate
from llama_index.core.query_pipeline import QueryPipeline, FnComponent

query_prompt_str = """\
Please expand the initial statement using the provided context from the Tesla 10K report.

{initial_statement}

"""
query_prompt_tmpl = PromptTemplate(query_prompt_str)

# MM model --> query prompt --> query engine
qp = QueryPipeline(
    modules={
        "mm_model": mm_model.as_query_component(
            partial={"image_documents": [image_doc]}
        ),
        "query_prompt": query_prompt_tmpl,
        "query_engine": query_engine,
    },
    verbose=True,
)
qp.add_chain(["mm_model", "query_prompt", "query_engine"])
rag_response = qp.run("Which Tesla Factory is shown in the image?")
```

**多模态 RAG（Retrieval-Augmented Generation，检索增强型生成）** 我们使用一个本地的 CLIP 嵌入模型对一组图像和文本进行索引。我们可以通过我们的 MultiModalVectorStoreIndex 将它们联合索引。

注：当前的实现将图像和文本混合在一起。你可以，也许应该为图像和文本分别定义不同的索引/检索器，这样可以让你为每种模态使用不同的嵌入/检索策略。

**构建多模态索引** 这是一个特殊的索引，它联合索引文本文档和图像文档。

我们使用一个本地的 CLIP 模型来嵌入图像/文本。

```python
from llama_index.core.indices.multi_modal.base import (
    MultiModalVectorStoreIndex,
)
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core import SimpleDirectoryReader, StorageContext
from llama_index.embeddings.clip import ClipEmbedding

import qdrant_client
from llama_index import (
    SimpleDirectoryReader,
)

# Create a local Qdrant vector store
client = qdrant_client.QdrantClient(path="qdrant_mm_db")

text_store = QdrantVectorStore(
    client=client, collection_name="text_collection"
)
image_store = QdrantVectorStore(
    client=client, collection_name="image_collection"
)
storage_context = StorageContext.from_defaults(
    vector_store=text_store, image_store=image_store
)

image_embed_model = ClipEmbedding()

# Create the MultiModal index
documents = SimpleDirectoryReader("./mixed_wiki/").load_data()
index = MultiModalVectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    image_embed_model=image_embed_model,
)

# Save it
# index.storage_context.persist(persist_dir="./storage")

# # Load it
# from llama_index import load_index_from_storage

# storage_context = StorageContext.from_defaults(
#     vector_store=text_store, persist_dir="./storage"
# )
# index = load_index_from_storage(storage_context, image_store=image_store)
```

```python
from llama_index.core.prompts import PromptTemplate
from llama_index.core.query_engine import SimpleMultiModalQueryEngine

qa_tmpl_str = (
    "Context information is below.\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Given the context information and not prior knowledge, "
    "answer the query.\n"
    "Query: {query_str}\n"
    "Answer: "
)
qa_tmpl = PromptTemplate(qa_tmpl_str)

query_engine = index.as_query_engine(
    multi_modal_llm=mm_model, text_qa_template=qa_tmpl
)

query_str = "Tell me more about the Porsche"
response = query_engine.query(query_str)
```

```python
from PIL import Image
import matplotlib.pyplot as plt
import os


def plot_images(image_paths):
    images_shown = 0
    plt.figure(figsize=(16, 9))
    for img_path in image_paths:
        if os.path.isfile(img_path):
            image = Image.open(img_path)

            plt.subplot(2, 3, images_shown + 1)
            plt.imshow(image)
            plt.xticks([])
            plt.yticks([])

            images_shown += 1
            if images_shown >= 9:
                break
```

```python
# show sources
from llama_index.core.response.notebook_utils import display_source_node

for text_node in response.metadata["text_nodes"]:
    display_source_node(text_node, source_length=200)
plot_images(
    [n.metadata["file_path"] for n in response.metadata["image_nodes"]]
)
```
